{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robin Hood Trading Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented here is a demonstration of how to use the robin.py module from the rh_trading repository on my github (https://github.com/michaelray1). The code is still very buggy, as you will see in the demonstration, so I need to work out a lot of those still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the robin.py module and numpy\n",
    "import robin as rb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a statistics object, then use that to create a metrics object\n",
    "stats = rb.statistics()\n",
    "metrics = rb.metrics(statistics = stats)\n",
    "\n",
    "#Now use these two to create a neural network object. You also need a username and password for this. Enter them below.\n",
    "un = ''\n",
    "pw = ''\n",
    "nn = rb.nn(metrics = metrics, statistics = stats, un = un, pw = pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've got all the necessary ingredients for the setup of our neural network. The last thing we need is some data to train the neural network. All we need to provide is the stock tickers for whatever stocks we are interested in analyzing. I've got a file saved here that has about 25 blue chip stocks (expensive, safe stocks) to use for this demonstration. Let's load in the tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IBM' 'XOM' 'CVX' 'PG' 'MMM' 'JNJ' 'MCD' 'WMT' 'KO' 'BA' 'CAT' 'JPM'\n",
      " 'HPQ' 'VZ' 'T' 'DD' 'MRK' 'DIS' 'HD' 'MSFT' 'AXP' 'BAC' 'PFE' 'GE' 'INTC'\n",
      " 'AA' 'C' 'GM']\n"
     ]
    }
   ],
   "source": [
    "#Load in blue chip stock tickers and print out what it looks like\n",
    "blue_chips = np.load('blue_chip_stock_tickers.npz')['arr_0']\n",
    "print(blue_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed collecting data for IBM (1 of 28 stocks done)\n",
      "completed collecting data for XOM (2 of 28 stocks done)\n",
      "completed collecting data for CVX (3 of 28 stocks done)\n",
      "completed collecting data for PG (4 of 28 stocks done)\n",
      "completed collecting data for MMM (5 of 28 stocks done)\n",
      "completed collecting data for JNJ (6 of 28 stocks done)\n",
      "completed collecting data for MCD (7 of 28 stocks done)\n",
      "completed collecting data for WMT (8 of 28 stocks done)\n",
      "completed collecting data for KO (9 of 28 stocks done)\n",
      "completed collecting data for BA (10 of 28 stocks done)\n",
      "completed collecting data for CAT (11 of 28 stocks done)\n",
      "completed collecting data for JPM (12 of 28 stocks done)\n",
      "completed collecting data for HPQ (13 of 28 stocks done)\n",
      "completed collecting data for VZ (14 of 28 stocks done)\n",
      "completed collecting data for T (15 of 28 stocks done)\n",
      "completed collecting data for DD (16 of 28 stocks done)\n",
      "completed collecting data for MRK (17 of 28 stocks done)\n",
      "completed collecting data for DIS (18 of 28 stocks done)\n",
      "completed collecting data for HD (19 of 28 stocks done)\n",
      "completed collecting data for MSFT (20 of 28 stocks done)\n",
      "completed collecting data for AXP (21 of 28 stocks done)\n",
      "completed collecting data for BAC (22 of 28 stocks done)\n",
      "completed collecting data for PFE (23 of 28 stocks done)\n",
      "completed collecting data for GE (24 of 28 stocks done)\n",
      "completed collecting data for INTC (25 of 28 stocks done)\n",
      "completed collecting data for AA (26 of 28 stocks done)\n",
      "completed collecting data for C (27 of 28 stocks done)\n",
      "completed collecting data for GM (28 of 28 stocks done)\n"
     ]
    }
   ],
   "source": [
    "#Use neural network functions to get training and testing data\n",
    "nn.get_tt_data(inputSymbols = blue_chips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing and training data is crucial to creating a successful neural network. In a sense, this is the main function that implements my trading strategy. Before I give a quick explanation of what the training/testing data is, let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group of 4 ones and zeros here represents one stock's data (if you count up all the rows shown, you get 28 which represents all the blue chip stocks we used). The rightmost value in each row is the \"expected output\". By default (you can change all these settings in the keyword arguments of nn.get_tt_data), this tells us whether the stock went up by 4% within 10 days of the end of the data that the neural network has access to. Essentially, we truncate the data that the neural network has access to. Then we use that shortened data set to try to predict the \"future\" (we have the \"future\" data already, we are using it to see if the neural network can make accurate predictions).\n",
    "\n",
    "Now, the other three numbers represent the different metrics. At current, there's only three metrics coded in, but I plan to expand this later. The three metrics coded in for now are bbands_bottom, ma_crossover, and rsi_crossover. bbands_bottom gives 1 if the stock's price is below one standard deviation from its mean, and 0 if it isn't that low. ma_crossover gives 1 if the short period (25 days) moving average is higher than the long period moving average (250 days). rsi_crossover is very similar to ma_crossover, but it uses the so called \"relative strength index\" as a measure of how the stock is doing.\n",
    "\n",
    "Once the neural network has the metrics, the idea is that it will weight each metric by a given amount (call these weights w_i). Then, it takes a look at the \"total prediction\" which is the sum of w_i * m_i where m_i is the i'th metric. Basically, we just look at a weighted sum of the metrics. Then a reLU activation function is applied where if the total prediction is higher than some specified value, then the neural network predicts a \"1\" which corresponds to \"buy the stock\". Anything below the specified value of the total prediction means that the neural network returns a \"0\" which corresponds to \"don't buy the stock\". \n",
    "\n",
    "Now, what the neural network is doing, is it's using the last column of our training data (the \"expected output\") to optimize the weights, w_i. By using the training/testing data to optimize the weights, the idea is that we can let the computer figure out which metrics are actually the best indicators of whether a stock will go up or down. Perhaps it's actually some funky combination of all the metrics that best indicates whether a stock will go up or down. Now let's see how to check the accuracy of our neural network and also use it to predict things about other stocks. This is where the code gets buggy and needs some work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Michaelray/opt/anaconda3/envs/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Build the neural network with default settings for size of layers\n",
    "nn.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples, validate on 6 samples\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/sample - loss: 0.0490 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 300us/sample - loss: 0.0458 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 253us/sample - loss: 0.0458 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 262us/sample - loss: 0.0459 - val_loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 304us/sample - loss: 0.0458 - val_loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 254us/sample - loss: 0.0456 - val_loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 327us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 319us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 326us/sample - loss: 0.0456 - val_loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 235us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 389us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 320us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 339us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 314us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 437us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 373us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 337us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 400us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 307us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 416us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 293us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 429us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 311us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 400us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 341us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 409us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 407us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 350us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 448us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 360us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 455us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 452us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 431us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 1ms/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 591us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 399us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 428us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 473us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 407us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 387us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 435us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 373us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 452us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 472us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 406us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 474us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 448us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 382us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 432us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 406us/sample - loss: 0.0455 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f823f787650>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the network\n",
    "nn.train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=&interval=day&span=year&bounds=regular\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d41410250ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/rh_trading/robin.py\u001b[0m in \u001b[0;36mtest_accuracy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mincorrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_tickers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_tickers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rh_trading/robin.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, stock, interval, span, num_bands, ma_short, ma_long, rsi_cutoff, rsi_strategy, days_before)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m#Calculate metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mbbands_bottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbands_bottom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdays_before\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0mma_crossover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma_crossover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdays_before\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mma_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mma_long\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mrsi_crossover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsi_crossover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdays_before\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsi_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrsi_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrsi_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rh_trading/robin.py\u001b[0m in \u001b[0;36mbbands_bottom\u001b[0;34m(self, stock_data, num_bands)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mbollinger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbollinger_bands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0minputSymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd_to_is\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rh_trading/robin.py\u001b[0m in \u001b[0;36mbollinger_bands\u001b[0;34m(self, stock_data)\u001b[0m\n\u001b[1;32m     94\u001b[0m         '''use the dictionary_data function above to put the data\n\u001b[1;32m     95\u001b[0m         \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mso\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m's easier to use'\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mclose_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'close_price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0minputSymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd_to_is\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rh_trading/robin.py\u001b[0m in \u001b[0;36mdictionary_data\u001b[0;34m(self, stock_data, data_point)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpast\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpresent\u001b[0m \u001b[0mending\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprevious\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m's data.'\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0minputSymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd_to_is\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mnum_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputSymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mdp_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#Test accuracy should tell us exactly how many the neural network correctly predicted from the testing/training set.\n",
    "#Be careful though, the network is trained on the same data set that we are using \"test_accuracy\" on.\n",
    "#So, if there is underlying bias in the data set, or if we overtrain on our data set, then test_accuracy will\n",
    "#tell us that our neural network has done very well, when it may not predict other stocks well at all.\n",
    "nn.test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (3,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6bd7a5469327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/rh_trading/robin.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, stock, interval, span, num_bands, ma_short, ma_long, rsi_cutoff, rsi_strategy, days_before)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbands_bottom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma_crossover\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsi_crossover\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;31m# generate symbolic tensors).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1060\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (3,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#We can also of course use the trained network to predict other stocks (this is what we ultimately want to do)\n",
    "#Not sure why this error is coming up. It seems to me that input_data is, in fact, of size (3,)\n",
    "nn.predict(stock='AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
