{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robin Hood Trading Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented here is a demonstration of how to use the robin.py module from the rh_trading repository on my github (https://github.com/michaelray1). The code is still very buggy, as you will see in the demonstration, so I need to work out a lot of those still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the robin.py module and numpy\n",
    "import robin as rb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a statistics object, then use that to create a metrics object\n",
    "stats = rb.statistics()\n",
    "metrics = rb.metrics(statistics = stats)\n",
    "\n",
    "#Now use these two to create a neural network object. You also need a username and password for this. Enter them below.\n",
    "un = ''\n",
    "pw = ''\n",
    "nn = rb.nn(metrics = metrics, statistics = stats, un = un, pw = pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've got all the necessary ingredients for the setup of our neural network. The last thing we need is some data to train the neural network. All we need to provide is the stock tickers for whatever stocks we are interested in analyzing. I've got a file saved here that has about 25 blue chip stocks (expensive, safe stocks) to use for this demonstration. Let's load in the tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IBM' 'XOM' 'CVX' 'PG' 'MMM' 'JNJ' 'MCD' 'WMT' 'KO' 'BA' 'CAT' 'JPM'\n",
      " 'HPQ' 'VZ' 'T' 'DD' 'MRK' 'DIS' 'HD' 'MSFT' 'AXP' 'BAC' 'PFE' 'GE' 'INTC'\n",
      " 'AA' 'C' 'GM']\n"
     ]
    }
   ],
   "source": [
    "#Load in blue chip stock tickers and print out what it looks like\n",
    "blue_chips = np.load('blue_chip_stock_tickers.npz')['arr_0']\n",
    "print(blue_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed collecting data for IBM (1 of 28 stocks done)\n",
      "completed collecting data for XOM (2 of 28 stocks done)\n",
      "completed collecting data for CVX (3 of 28 stocks done)\n",
      "completed collecting data for PG (4 of 28 stocks done)\n",
      "completed collecting data for MMM (5 of 28 stocks done)\n",
      "completed collecting data for JNJ (6 of 28 stocks done)\n",
      "completed collecting data for MCD (7 of 28 stocks done)\n",
      "completed collecting data for WMT (8 of 28 stocks done)\n",
      "completed collecting data for KO (9 of 28 stocks done)\n",
      "completed collecting data for BA (10 of 28 stocks done)\n",
      "completed collecting data for CAT (11 of 28 stocks done)\n",
      "completed collecting data for JPM (12 of 28 stocks done)\n",
      "completed collecting data for HPQ (13 of 28 stocks done)\n",
      "completed collecting data for VZ (14 of 28 stocks done)\n",
      "completed collecting data for T (15 of 28 stocks done)\n",
      "completed collecting data for DD (16 of 28 stocks done)\n",
      "completed collecting data for MRK (17 of 28 stocks done)\n",
      "completed collecting data for DIS (18 of 28 stocks done)\n",
      "completed collecting data for HD (19 of 28 stocks done)\n",
      "completed collecting data for MSFT (20 of 28 stocks done)\n",
      "completed collecting data for AXP (21 of 28 stocks done)\n",
      "completed collecting data for BAC (22 of 28 stocks done)\n",
      "completed collecting data for PFE (23 of 28 stocks done)\n",
      "completed collecting data for GE (24 of 28 stocks done)\n",
      "completed collecting data for INTC (25 of 28 stocks done)\n",
      "completed collecting data for AA (26 of 28 stocks done)\n",
      "completed collecting data for C (27 of 28 stocks done)\n",
      "completed collecting data for GM (28 of 28 stocks done)\n"
     ]
    }
   ],
   "source": [
    "#Use neural network functions to get training and testing data\n",
    "nn.get_tt_data(inputSymbols = blue_chips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing and training data is crucial to creating a successful neural network. In a sense, this is the main function that implements my trading strategy. Before I give a quick explanation of what the training/testing data is, let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group of 4 ones and zeros here represents one stock's data (if you count up all the rows shown, you get 28 which represents all the blue chip stocks we used). The rightmost value in each row is the \"expected output\". By default (you can change all these settings in the keyword arguments of nn.get_tt_data), this tells us whether the stock went up by 4% within 10 days of the end of the data that the neural network has access to. Essentially, we truncate the data that the neural network has access to. Then we use that shortened data set to try to predict the \"future\" (we have the \"future\" data already, we are using it to see if the neural network can make accurate predictions).\n",
    "\n",
    "Now, the other three numbers represent the different metrics. At current, there's only three metrics coded in, but I plan to expand this later. The three metrics coded in for now are bbands_bottom, ma_crossover, and rsi_crossover. bbands_bottom gives 1 if the stock's price is below one standard deviation from its mean, and 0 if it isn't that low. ma_crossover gives 1 if the short period (25 days) moving average is higher than the long period moving average (250 days). rsi_crossover is very similar to ma_crossover, but it uses the so called \"relative strength index\" as a measure of how the stock is doing.\n",
    "\n",
    "Once the neural network has the metrics, the idea is that it will weight each metric by a given amount (call these weights w_i). Then, it takes a look at the \"total prediction\" which is the sum of w_i * m_i where m_i is the i'th metric. Basically, we just look at a weighted sum of the metrics. Then a reLU activation function is applied where if the total prediction is higher than some specified value, then the neural network predicts a \"1\" which corresponds to \"buy the stock\". Anything below the specified value of the total prediction means that the neural network returns a \"0\" which corresponds to \"don't buy the stock\". \n",
    "\n",
    "Now, what the neural network is doing, is it's using the last column of our training data (the \"expected output\") to optimize the weights, w_i. By using the training/testing data to optimize the weights, the idea is that we can let the computer figure out which metrics are actually the best indicators of whether a stock will go up or down. Perhaps it's actually some funky combination of all the metrics that best indicates whether a stock will go up or down. Now let's see how to check the accuracy of our neural network and also use it to predict things about other stocks. This is where the code gets buggy and needs some work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the neural network with default settings for size of layers\n",
    "nn.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples, validate on 6 samples\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/sample - loss: 0.0944 - val_loss: 0.1667\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 244us/sample - loss: 0.0923 - val_loss: 0.1667\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 263us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 250us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 302us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 271us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 288us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 337us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 250us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 303us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 252us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 267us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 289us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 242us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 251us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 260us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 260us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 262us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 280us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 290us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 268us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 347us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 265us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 268us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 289us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 281us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 276us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 284us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 326us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 306us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 321us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 365us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 328us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 236us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 260us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 263us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 268us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 252us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 257us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 247us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 331us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 313us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 276us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 262us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 324us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 280us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 297us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 251us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 243us/sample - loss: 0.0909 - val_loss: 0.1667\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 244us/sample - loss: 0.0909 - val_loss: 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7febe379af90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the network\n",
    "nn.train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our neural network is trained, we can use it to predict another stock (Apple, for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(['AAPL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0 here tells us that the neural network says we should not buy. Really, it's saying it does not predict a 4% increase within the next 10 days (the percentage and number of days here can be changed). You can change the 4 and 10 using extra arguments in the nn.predict function. Be careful though, because the network was trained using data assuming we were shooting for a 4% increase in the next 10 days (or whatever percentage and number of days you change it to). So, if you want to change the percentage and number of days, you'd better use nn.get_tt_data and nn.train_network (in that order) with the new percentage and number of days. That way, you get new testing/training data using the new criteria and you train the network using that new data.\n",
    "\n",
    "Now, what we should be more interested in is the ability of our neural network to make predictions in large numbers, because we probably want a diverse investment portfolio. Over time, we hope that we make enough good purchases so that we make money and hopefully beat the market. To look at this, we use the nn.test_accuracy() function. This uses the testing data (in this case, the testing data uses only 6 stocks) to see how many correct predictions the neural network makes. When using this code, you probably want to use a much larger number of stocks to test and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making a prediction for ...      PFE\n",
      "Correctly predicted!\n",
      "making a prediction for ...      GE\n",
      "Correctly predicted!\n",
      "making a prediction for ...      INTC\n",
      "Correctly predicted!\n",
      "making a prediction for ...      AA\n",
      "Correctly predicted!\n",
      "making a prediction for ...      C\n",
      "Incorrectly predicted\n",
      "making a prediction for ...      GM\n",
      "Correctly predicted!\n",
      "\n",
      "\n",
      "5/6 correctly predicted\n"
     ]
    }
   ],
   "source": [
    "#Test accuracy should tell us exactly how many the neural network correctly predicted from the testing/training set.\n",
    "#Be careful though, the network is trained on the same data set that we are using \"test_accuracy\" on.\n",
    "#So, if there is underlying bias in the data set, or if we overtrain on our data set, then test_accuracy will\n",
    "#tell us that our neural network has done very well, when it may not predict other stocks well at all.\n",
    "nn.test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
